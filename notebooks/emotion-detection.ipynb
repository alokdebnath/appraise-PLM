{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6cf458-952d-4a86-9f24-31c7510a8aa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text_id</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>suddenness</th>\n",
       "      <th>familiarity</th>\n",
       "      <th>predict_event</th>\n",
       "      <th>pleasantness</th>\n",
       "      <th>unpleasantness</th>\n",
       "      <th>goal_relevance</th>\n",
       "      <th>chance_responsblt</th>\n",
       "      <th>...</th>\n",
       "      <th>self_control</th>\n",
       "      <th>other_control</th>\n",
       "      <th>chance_control</th>\n",
       "      <th>accept_conseq</th>\n",
       "      <th>standards</th>\n",
       "      <th>social_norms</th>\n",
       "      <th>attention</th>\n",
       "      <th>not_consider</th>\n",
       "      <th>effort</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3573</td>\n",
       "      <td>I was told “I love you” by a special person</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>51199</td>\n",
       "      <td>I gave birth to my son.</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6601</td>\n",
       "      <td>When a student told me to ‘fuck off’ to my face</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>no-emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3727</td>\n",
       "      <td>When I’m doing my day to day routine.</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>no-emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>52291</td>\n",
       "      <td>i failed a subject.</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>4315</td>\n",
       "      <td>760</td>\n",
       "      <td>I had to go into my brothers flooded basement ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4316</th>\n",
       "      <td>4316</td>\n",
       "      <td>5121</td>\n",
       "      <td>When my dad cut me from his life</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>4317</td>\n",
       "      <td>4506</td>\n",
       "      <td>I felt fear when walking home alone on Wednesd...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>4318</td>\n",
       "      <td>52508</td>\n",
       "      <td>my husband and my daughter came to meet me fro...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>4319</td>\n",
       "      <td>31036</td>\n",
       "      <td>My grandma was suddenly in the hospital and no...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4320 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  text_id                                     generated_text  \\\n",
       "0              0     3573        I was told “I love you” by a special person   \n",
       "1              1    51199                            I gave birth to my son.   \n",
       "2              2     6601    When a student told me to ‘fuck off’ to my face   \n",
       "3              3     3727              When I’m doing my day to day routine.   \n",
       "4              4    52291                                i failed a subject.   \n",
       "...          ...      ...                                                ...   \n",
       "4315        4315      760  I had to go into my brothers flooded basement ...   \n",
       "4316        4316     5121                   When my dad cut me from his life   \n",
       "4317        4317     4506  I felt fear when walking home alone on Wednesd...   \n",
       "4318        4318    52508  my husband and my daughter came to meet me fro...   \n",
       "4319        4319    31036  My grandma was suddenly in the hospital and no...   \n",
       "\n",
       "      suddenness  familiarity  predict_event  pleasantness  unpleasantness  \\\n",
       "0              2            2              4             5               1   \n",
       "1              2            3              5             3               3   \n",
       "2              5            1              1             1               5   \n",
       "3              4            2              2             1               3   \n",
       "4              2            3              4             1               5   \n",
       "...          ...          ...            ...           ...             ...   \n",
       "4315           3            1              1             1               5   \n",
       "4316           2            1              1             1               5   \n",
       "4317           2            4              4             1               5   \n",
       "4318           3            4              2             4               1   \n",
       "4319           5            1              1             1               5   \n",
       "\n",
       "      goal_relevance  chance_responsblt  ...  self_control  other_control  \\\n",
       "0                  5                  3  ...             5              5   \n",
       "1                  5                  4  ...             1              2   \n",
       "2                  1                  1  ...             1              5   \n",
       "3                  2                  1  ...             4              1   \n",
       "4                  5                  3  ...             4              4   \n",
       "...              ...                ...  ...           ...            ...   \n",
       "4315               1                  4  ...             4              1   \n",
       "4316               5                  1  ...             1              5   \n",
       "4317               1                  1  ...             3              3   \n",
       "4318               2                  1  ...             1              5   \n",
       "4319               3                  5  ...             1              1   \n",
       "\n",
       "      chance_control  accept_conseq  standards  social_norms  attention  \\\n",
       "0                  5              4          1             1          4   \n",
       "1                  4              4          1             1          5   \n",
       "2                  1              1          3             5          5   \n",
       "3                  4              1          1             1          1   \n",
       "4                  4              3          4             1          4   \n",
       "...              ...            ...        ...           ...        ...   \n",
       "4315               5              1          4             1          5   \n",
       "4316               1              1          4             5          5   \n",
       "4317               1              3          5             1          5   \n",
       "4318               1              2          1             1          1   \n",
       "4319               5              3          3             1          4   \n",
       "\n",
       "      not_consider  effort     emotion  \n",
       "0                4       3         joy  \n",
       "1                1       5         joy  \n",
       "2                3       5  no-emotion  \n",
       "3                2       4  no-emotion  \n",
       "4                4       4       shame  \n",
       "...            ...     ...         ...  \n",
       "4315             4       3        fear  \n",
       "4316             5       5       anger  \n",
       "4317             5       4        fear  \n",
       "4318             1       1    surprise  \n",
       "4319             3       5     sadness  \n",
       "\n",
       "[4320 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb39c7-72b4-4180-b2a5-221b8a92fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Define the custom dataset class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length, label_encoder):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing 'sentence' and 'label' columns.\n",
    "            tokenizer: Tokenizer for text data.\n",
    "            max_length (int): Maximum sequence length for tokenization.\n",
    "            label_encoder: Label encoder for converting labels to numeric.\n",
    "        \"\"\"\n",
    "        self.sentences = dataframe['generated_text'].tolist()\n",
    "        self.labels = dataframe['emotion'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_encoder = label_encoder\n",
    "        self.encoded_labels = label_encoder.transform(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the sentence\n",
    "        inputs = self.tokenizer(\n",
    "            self.sentences[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        # Get the label\n",
    "        label = torch.tensor(self.encoded_labels[idx], dtype=torch.long)\n",
    "        # Flatten tokenizer outputs (remove batch dimension)\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        return inputs, label\n",
    "\n",
    "# Prepare label encoder\n",
    "def prepare_label_encoder(dataframe, label):\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(dataframe[label])\n",
    "    return label_encoder\n",
    "\n",
    "# Data preparation function\n",
    "def prepare_dataloaders(train_path, val_path, tokenizer, batch_size=32, max_length=128):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_path (str): Path to the training CSV file.\n",
    "        val_path (str): Path to the validation CSV file.\n",
    "        tokenizer: Tokenizer for text data.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        max_length (int): Maximum sequence length for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader: Dataloaders for training and validation data.\n",
    "        label_encoder: Fitted label encoder for decoding emotion labels.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    emo_label = 'emotion'\n",
    "    \n",
    "    # Prepare label encoder\n",
    "    label_encoder = prepare_label_encoder(train_df, emo_label)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = EmotionDataset(train_df, tokenizer, max_length, label_encoder)\n",
    "    val_dataset = EmotionDataset(val_df, tokenizer, max_length, label_encoder)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f377a5f5-76e1-4f77-9857-e6e7afa3aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, labels in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(**batch)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training loop with mixed precision and gradient checkpointing\n",
    "def train_model(model, dataloader, optimizer, scheduler, criterion, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(**batch)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Model and tokenizer setup\n",
    "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=NUM_EMOTIONS)\n",
    "    model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "    \n",
    "    # Mixed precision training setup\n",
    "    scaler = GradScaler()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72327c3-222f-460c-96c9-f7e59cf9fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define the Appraisal MLP model\n",
    "class AppraisalMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_emotions):\n",
    "        super(AppraisalMLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_emotions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Training loop with mixed precision and gradient checkpointing\n",
    "def train_appraisal_model(model, dataloader, optimizer, criterion, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for appraisal_inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            appraisal_inputs = appraisal_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast():  # Mixed precision context\n",
    "                outputs = model(appraisal_inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(dataloader)\n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Example of creating the optimizer and criterion\n",
    "input_dim = 21  # Change to match your dataset's number of appraisal dimensions\n",
    "num_emotions = 6  # Change to match the number of emotion classes\n",
    "model = AppraisalMLP(input_dim=input_dim, num_emotions=num_emotions)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assume `train_dataloader` and `val_dataloader` are already defined\n",
    "train_appraisal_model(model, train_dataloader, optimizer, criterion, device='cuda', num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a436618-71c3-41c8-b656-8392816b8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_emotions, appraisal_input_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.text_model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=num_emotions)\n",
    "        self.text_model.gradient_checkpointing_enable()  # Enable gradient checkpointing for the text model\n",
    "        self.appraisal_model = AppraisalMLP(input_dim=appraisal_input_dim, num_emotions=num_emotions)\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(num_emotions * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_emotions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_inputs, appraisal_inputs):\n",
    "        text_outputs = self.text_model(**text_inputs).logits\n",
    "        appraisal_outputs = self.appraisal_model(appraisal_inputs)\n",
    "        combined = torch.cat((text_outputs, appraisal_outputs), dim=1)\n",
    "        return self.fc_combined(combined)\n",
    "\n",
    "# Combined training with mixed precision\n",
    "def train_combined_model(model, text_dataloader, appraisal_dataloader, optimizer, scheduler, criterion, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for (text_batch, text_labels), appraisal_inputs in zip(text_dataloader, appraisal_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            text_batch = {k: v.to(device) for k, v in text_batch.items()}\n",
    "            text_labels = text_labels.to(device)\n",
    "            appraisal_inputs = appraisal_inputs.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(text_batch, appraisal_inputs)\n",
    "                loss = criterion(outputs, text_labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187bcbb-b45f-4e46-a795-e44f048e430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, labels in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(**batch)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49532566-ec15-4659-b5f8-196dd2f1c20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6acf8c-63fa-4682-a909-21b0d2069557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
